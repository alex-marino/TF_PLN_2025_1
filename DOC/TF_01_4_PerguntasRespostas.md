# Tema 4 â€” Sistema de Perguntas e Respostas (QA)

## ğŸ¯ Objetivo EspecÃ­fico

Treinar e/ou aplicar modelos baseados em transformers para construir um **sistema de perguntas e respostas** (Question Answering, QA) capaz de localizar ou gerar respostas com base em um **corpus especÃ­fico de texto**. A tarefa pode assumir duas formas principais:
- **Extractiva**: a resposta Ã© um trecho exato do texto fornecido.
- **Abstrativa**: a resposta Ã© gerada com base no entendimento do texto, podendo ter reescrita.

---

## ğŸ“š SugestÃµes de Datasets PÃºblicos em PortuguÃªs

### 1. **SQuAD traduzido (portuguesequad)**
- **DescriÃ§Ã£o**: VersÃ£o traduzida do SQuAD 1.1 com perguntas e trechos contextuais em portuguÃªs.
- **Fonte**: [https://github.com/portuguese-nlp/portuguese-squad](https://github.com/portuguese-nlp/portuguese-squad)
- **Formato**: JSON compatÃ­vel com HuggingFace.

### 2. **Piaf (pt-br subset)**
- **DescriÃ§Ã£o**: Conjunto de perguntas sobre textos retirados de Wikipedia e artigos, disponÃ­vel em mÃºltiplas lÃ­nguas.
- **Fonte**: [Hugging Face Datasets - Piaf](https://huggingface.co/datasets/etalab-ia/piaf)

### 3. **Corpus personalizado**
- **DescriÃ§Ã£o**: Pode-se criar um corpus com textos pÃºblicos (leis, documentos institucionais, etc.) e manualmente anotar pares de perguntas e respostas.

---

## ğŸ“¦ Requisitos Objetivos para o Projeto

1. Escolher um corpus ou dataset com **contexto e perguntas/respostas anotadas**
2. Implementar um modelo de QA extractivo ou abstrativo
3. PrÃ©-processar os textos (tokenizaÃ§Ã£o, truncamento, etc.)
4. Realizar o treinamento (opcional) com `Trainer` da Hugging Face
5. Implementar um sistema interativo (ex.: usuÃ¡rio faz pergunta â†’ resposta gerada)
6. Avaliar a performance com mÃ©tricas padrÃ£o de QA
7. RelatÃ³rio com anÃ¡lise quantitativa e exemplos qualitativos de perguntas e respostas

---

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

### 1. **Exact Match (EM)**
- Percentual de respostas exatamente iguais Ã  referÃªncia.

### 2. **F1-score (overlap de tokens)**
- Mede sobreposiÃ§Ã£o parcial entre resposta gerada e esperada (mais tolerante que EM).

### 3. **Human Evaluation (opcional)**
- AvaliaÃ§Ã£o qualitativa da relevÃ¢ncia e precisÃ£o da resposta.

---

## ğŸ“ SugestÃ£o de Roteiro TÃ©cnico

1. **Carregar e explorar o dataset (ex.: portuguesequad)**
2. **Tokenizar usando modelo de QA (ex.: BERT, RoBERTa, T5)**
3. **PrÃ©-processar com mapeamento de contextos e spans de resposta**
4. **Aplicar fine-tuning com `Trainer` ou `pipeline` de QA**
5. **Avaliar no conjunto de teste com `exact_match` e `f1`**
6. **Montar interface de exemplo para testar perguntas do usuÃ¡rio**
7. **Apresentar exemplos de sucesso e falha**

---

## ğŸ§  Modelos Recomendados

- `mrm8488/bert-base-portuguese-cased-finetuned-squad-v1` (QA em portuguÃªs)
- `neuralmind/bert-base-portuguese-cased` (com fine-tuning)
- `t5-base` ou `unicamp-dl/ptt5-base-portuguese-vocab` (para QA generativo)
- `deepset/roberta-base-squad2` (boa base para adaptaÃ§Ã£o)

---

