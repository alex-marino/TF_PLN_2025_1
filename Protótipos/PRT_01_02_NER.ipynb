{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Fine-Tuning de NER com Transformers em Portugu√™s üáßüá∑\n",
    "\n",
    "Este notebook mostra como treinar um modelo BERT para Reconhecimento de Entidades Nomeadas (NER) usando a biblioteca Hugging Face.\n"
   ],
   "id": "ae1465a90de602a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìÅ 2. Preparar dataset de exemplo em formato BIO\n",
   "id": "e57e3fd29f52694f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Exemplo did√°tico: frases tokenizadas com r√≥tulos BIO\n",
    "data = {\n",
    "    \"tokens\": [\n",
    "        [\"Jo√£o\", \"mora\", \"em\", \"S√£o\", \"Paulo\", \".\"],\n",
    "        [\"Maria\", \"trabalha\", \"na\", \"Google\", \".\"]\n",
    "    ],\n",
    "    \"ner_tags\": [\n",
    "        [1, 0, 0, 2, 3, 0],  # Jo√£o (B-PER), S√£o (B-LOC), Paulo (I-LOC)\n",
    "        [1, 0, 0, 4, 0]      # Maria (B-PER), Google (B-ORG)\n",
    "    ]\n",
    "}\n",
    "\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\"]\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = DatasetDict({\"train\": dataset, \"validation\": dataset})\n"
   ],
   "id": "9e9442e244c6beaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üî§ 3. Tokeniza√ß√£o com alinhamento de r√≥tulos",
   "id": "610101314f6210d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=False)\n"
   ],
   "id": "206ddf6c9f3b6c49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üß† 4. Carregar modelo pr√©-treinado para Token Classification",
   "id": "8b8de4da34140fea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list)\n",
    ")\n"
   ],
   "id": "444fea82bde90210"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list)\n",
    ")\n"
   ],
   "id": "4f544f085ca5e742"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚öôÔ∏è 5. Configurar treinamento",
   "id": "23d37cc07d3308b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ],
   "id": "dc99cb9734cd4689"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìä 6. M√©tricas com seqeval",
   "id": "10fe876cdcfbd8b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)\n",
    "\n",
    "    true_labels = [[label_list[l] for l in example if l != -100] for example in labels]\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions)\n",
    "    }\n"
   ],
   "id": "e2f2e1c5e7b85a96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üèãÔ∏è 7. Treinar o modelo",
   "id": "c623e3271f8115db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "1373e4b2be27c4bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚úÖ 8. Avaliar no conjunto de valida√ß√£o",
   "id": "f5f2108754335d5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()\n",
   "id": "af64b0686e72f684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå Observa√ß√µes\n",
    "O exemplo usa apenas duas frases fict√≠cias. Para uso real, substitua o dataset por dados reais anotados em BIO.\n",
    "\n",
    "A m√©trica seqeval fornece avalia√ß√£o padr√£o para tarefas de NER com m√∫ltiplas entidades.\n",
    "\n",
    "Voc√™ pode exportar o modelo final com trainer.save_model()."
   ],
   "id": "31371664ab4a9f74"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
