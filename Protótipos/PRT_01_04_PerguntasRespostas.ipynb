{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Roadmap ‚Äî Sistema de Perguntas e Respostas com Transformers\n",
    "\n",
    "---"
   ],
   "id": "419dceb9bf681c28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ü§ó Pipeline de QA com modelo treinado em portugu√™s",
   "id": "6d65ac3f5e8a5c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Modelo BERT em portugu√™s treinado em QA\n",
    "qa_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"mrm8488/bert-base-portuguese-cased-finetuned-squad-v1\",\n",
    "    tokenizer=\"mrm8488/bert-base-portuguese-cased-finetuned-squad-v1\"\n",
    ")\n",
    "\n",
    "contexto = \"\"\"\n",
    "A Amaz√¥nia √© a maior floresta tropical do mundo. Ela cobre nove pa√≠ses e representa uma das maiores reservas de biodiversidade do planeta.\n",
    "\"\"\"\n",
    "\n",
    "pergunta = \"Qual √© a maior floresta tropical do mundo?\"\n",
    "\n",
    "resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n"
   ],
   "id": "d6f3ec50ae1124df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚úÖ Parte 2 ‚Äî Fine-Tuning com modelo extractivo (SQuAD em portugu√™s)",
   "id": "d88fa21c95635701"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:1000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n"
   ],
   "id": "8483d7a069511be4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üî§ Tokeniza√ß√£o e alinhamento de contexto/pergunta",
   "id": "1ddfd70a223cddd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess, batched=True)\n"
   ],
   "id": "3bcdbde3447bf9e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üß† Carregar modelo para QA extractivo",
   "id": "baca0da0383bc575"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n"
   ],
   "id": "f0de9b80356a7ccd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚öôÔ∏è Configurar treinamento",
   "id": "54fb973df94e8445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./qa-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ],
   "id": "f48abf3a88b56341"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üß™ Avalia√ß√£o com F1 e EM",
   "id": "e3d1e5dd381e7f2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n"
   ],
   "id": "926a0f19836a74cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üèãÔ∏è Treinamento com Trainer",
   "id": "a5f683f401c641c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, DefaultDataCollator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "ae82fb054be634ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚úÖ Avalia√ß√£o final",
   "id": "e41094a91bb2d24f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()\n",
   "id": "28440a12dbeae5c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå Observa√ß√µes\n",
    "* A tarefa de QA extractivo assume que a resposta est√° contida literalmente no contexto.\n",
    "\n",
    "* Para QA generativo (ex.: com T5), use o prefixo \"pergunte:\" e modelo de gera√ß√£o seq2seq.\n",
    "\n",
    "* M√©tricas como Exact Match (EM) e F1-score s√£o as mais usadas para QA."
   ],
   "id": "a030d0c739e8bc42"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
